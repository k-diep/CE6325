{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hls4ml version: 0.9.0.dev5+g033d4382\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "print(\"hls4ml version:\", hls4ml.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import qkeras\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "#from callbacks import all_callbacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "trim = 4\n",
    "x_train_ = np.round(x_train[:, trim:28-trim, trim:28-trim].reshape(x_train.shape[0],-1))\n",
    "x_test_ = np.round(x_test[:, trim:28-trim, trim:28-trim].reshape(x_test.shape[0],-1))\n",
    "input_shape = x_train_.shape[1:]\n",
    "print(input_shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "((60000, 400), (10000, 400))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_.shape, x_test_.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fc1 (QDense)                (None, 10)                4010      \n",
      "                                                                 \n",
      " relu1 (QActivation)         (None, 10)                0         \n",
      "                                                                 \n",
      " fc2 (QDense)                (None, 10)                110       \n",
      "                                                                 \n",
      " relu2 (QActivation)         (None, 10)                0         \n",
      "                                                                 \n",
      " fc3 (QDense)                (None, 10)                110       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4230 (16.52 KB)\n",
      "Trainable params: 4230 (16.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.Input(shape=input_shape, name=\"input0\"))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    QDense(\n",
    "        10,\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        10,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        num_classes,\n",
    "        name='fc3',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 1.5562 - accuracy: 0.6201 - val_loss: 1.0976 - val_accuracy: 0.7970\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.9417 - accuracy: 0.8202 - val_loss: 0.7315 - val_accuracy: 0.8848\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.8688 - val_loss: 0.5813 - val_accuracy: 0.9000\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.5969 - accuracy: 0.8833 - val_loss: 0.5143 - val_accuracy: 0.9087\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.8889 - val_loss: 0.4766 - val_accuracy: 0.9095\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.8923 - val_loss: 0.4605 - val_accuracy: 0.9123\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.5062 - accuracy: 0.8954 - val_loss: 0.4568 - val_accuracy: 0.9117\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4950 - accuracy: 0.8985 - val_loss: 0.4553 - val_accuracy: 0.9125\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4882 - accuracy: 0.9012 - val_loss: 0.4428 - val_accuracy: 0.9168\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4821 - accuracy: 0.9021 - val_loss: 0.4584 - val_accuracy: 0.9088\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.9030 - val_loss: 0.4370 - val_accuracy: 0.9163\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.9032 - val_loss: 0.4359 - val_accuracy: 0.9155\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4697 - accuracy: 0.9049 - val_loss: 0.4335 - val_accuracy: 0.9175\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.9053 - val_loss: 0.4373 - val_accuracy: 0.9177\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4634 - accuracy: 0.9056 - val_loss: 0.4307 - val_accuracy: 0.9170\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x25f28b8e7f0>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model.fit(x_train_, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.47468385100364685\n",
      "Test accuracy: 0.8988000154495239\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "save_model(model, \"qModel.keras\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input0, layer type: InputLayer, input shapes: [[None, 400]], output shape: [None, 400]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 400]], output shape: [None, 10]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "-----------------------------------\n",
      "{'Model': {'Precision': 'fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}, 'LayerName': {'input0': {'Trace': False, 'Precision': {'result': 'ap_uint<1>'}}, 'fc1': {'Trace': False, 'Precision': {'result': 'fixed<16,6>', 'weight': 'fixed<6,1>', 'bias': 'fixed<6,1>'}}, 'fc1_linear': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}}, 'relu1': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT>'}}, 'fc2': {'Trace': False, 'Precision': {'result': 'fixed<16,6>', 'weight': 'fixed<6,1>', 'bias': 'fixed<6,1>'}}, 'fc2_linear': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}}, 'relu2': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT>'}}, 'fc3': {'Trace': False, 'Precision': {'result': 'fixed<16,6>', 'weight': 'fixed<6,1>', 'bias': 'fixed<6,1>'}}, 'fc3_linear': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}}, 'softmax': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}, 'exp_table_t': 'ap_fixed<18,8>', 'inv_table_t': 'ap_fixed<18,4>'}}}\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input0, layer type: InputLayer, input shapes: [[None, 400]], output shape: [None, 400]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 400]], output shape: [None, 10]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'C:\\Users\\Tim\\PycharmProjects\\rs_final\\model_1\\hls4ml_prj\\firmware\\myproject-eC7Bc7Fa.so' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 12\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-----------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m hls_model \u001B[38;5;241m=\u001B[39m hls4ml\u001B[38;5;241m.\u001B[39mconverters\u001B[38;5;241m.\u001B[39mconvert_from_keras_model(\n\u001B[0;32m      9\u001B[0m     model, hls_config\u001B[38;5;241m=\u001B[39mconfig, output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_1/hls4ml_prj\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     10\u001B[0m     part\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxcu250-figd2104-2L-e\u001B[39m\u001B[38;5;124m'\u001B[39m, backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVitis\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     11\u001B[0m )\n\u001B[1;32m---> 12\u001B[0m \u001B[43mhls_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ti\\lib\\site-packages\\hls4ml\\model\\graph.py:664\u001B[0m, in \u001B[0;36mModelGraph.compile\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    662\u001B[0m     dlclose_func\u001B[38;5;241m.\u001B[39mrestype \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mc_int\n\u001B[0;32m    663\u001B[0m     dlclose_func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_top_function_lib\u001B[38;5;241m.\u001B[39m_handle)\n\u001B[1;32m--> 664\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_top_function_lib \u001B[38;5;241m=\u001B[39m \u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcdll\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibrary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlib_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ti\\lib\\ctypes\\__init__.py:451\u001B[0m, in \u001B[0;36mLibraryLoader.LoadLibrary\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLoadLibrary\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[1;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dlltype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ti\\lib\\ctypes\\__init__.py:373\u001B[0m, in \u001B[0;36mCDLL.__init__\u001B[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001B[0m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_FuncPtr \u001B[38;5;241m=\u001B[39m _FuncPtr\n\u001B[0;32m    372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 373\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m \u001B[43m_dlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    375\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m handle\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Could not find module 'C:\\Users\\Tim\\PycharmProjects\\rs_final\\model_1\\hls4ml_prj\\firmware\\myproject-eC7Bc7Fa.so' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "config['LayerName']['input0']['Precision'] = {'result': 'ap_uint<1>'}  # Set input precision to 1-bit\n",
    "print(\"-----------------------------------\")\n",
    "print(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_1/hls4ml_prj',\n",
    "    part='xcu250-figd2104-2L-e', backend='Vitis'\n",
    ")\n",
    "hls_model.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model.write()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n  'EstimatedClockPeriod': '4.344',\n  'BestLatency': '10',\n  'WorstLatency': '10',\n  'IntervalMin': '1',\n  'IntervalMax': '1',\n  'BRAM_18K': '12',\n  'DSP': '10',\n  'FF': '2629',\n  'LUT': '61606',\n  'URAM': '0',\n  'AvailableBRAM_18K': '5376',\n  'AvailableDSP': '12288',\n  'AvailableFF': '3456000',\n  'AvailableLUT': '1728000',\n  'AvailableURAM': '1280'}}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PATH'] += os.pathsep + 'D:/ProgramFiles/Xilinx/Vitis_HLS/2023.2/bin'\n",
    "hls_model.build(csim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'hls4ml': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}